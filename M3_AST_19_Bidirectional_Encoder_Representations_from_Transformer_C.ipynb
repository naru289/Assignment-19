{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-19/blob/main/M3_AST_19_Bidirectional_Encoder_Representations_from_Transformer_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 19: Natural Language Processing - IV\n",
        "### (BERT - Bidirectional Encoder Representations from Transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will:\n",
        " \n",
        "*  understand about the BERT model\n",
        "*  understand about preprocessing required for question answering task using BERT model\n",
        "*  use pretrained BERT model for question answering task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_19_Bidirectional_Encoder_Representations_from_Transformer_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "The aim of this assignment is to study the use of BERT model for question answering task using a pretrained BERT model. For something like text classification, you definitely want to fine-tune BERT on your own dataset. For question answering, however, you may be able to get decent results using a model that's already been fine-tuned on the SQuAD benchmark. The task posed by the SQuAD (Stanford Question Answering Dataset) benchmark contains a question, and a passage of text containing the answer, BERT needs to highlight the \"span\" of text corresponding to the correct answer. In this assignment, we'll be downloading a model that's already been fine-tuned for question answering, and try it out on our own text and see that it performs well on text that wasn't in the SQuAD dataset.\n"
      ],
      "metadata": {
        "id": "F8jHOEg7h0oA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVM_u_Czx_jZ"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "POgC5ToSo2HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xN5f1bxf6K_"
      },
      "source": [
        "### BERT Input Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctum5SK6f9uP"
      },
      "source": [
        "Let us understand about BERT input format. To feed a QA task into BERT, we pack both the question and the reference text into the input. Look into the below diagram shows the input format in a more comprehensive way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaPCFHgSOO37"
      },
      "source": [
        "\n",
        "[![Input format for QA](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-SS0PucOQdt"
      },
      "source": [
        "\n",
        "The two pieces of text are separated by the special `[SEP]` token. Further, the BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer. \n",
        "\n",
        "Now, let us understand about start and end token classifiers that helps the algorithm to mark the piece of text that we want to find."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs31dcrPg5Tg"
      },
      "source": [
        "### Start & End Token Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOdUa9Wg-Uv"
      },
      "source": [
        "BERT needs to highlight a \"span\" of text containing the answer--this is represented as simply predicting which token marks the start of the answer, and which token marks the end.\n",
        "\n",
        "![Start token classification](http://www.mccormickml.com/assets/BERT/SQuAD/start_token_classification.png)\n",
        "\n",
        "For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue \"start\" rectangle in the above illustration) which it applies to every word.\n",
        "\n",
        "After taking the dot product between the output embeddings and the 'start' weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick.\n",
        "\n",
        "We repeat this process for the end token--we have a separate weight vector this.\n",
        "\n",
        "![End token classification](http://www.mccormickml.com/assets/BERT/SQuAD/end_token_classification.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpCLgVCkki5"
      },
      "source": [
        "Further, we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text.\n",
        "\n",
        "If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See [run_squad.py](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) in the `transformers` library. However, you may find that the below \"fine-tuned-on-squad\" model already does a good job, even if your text is from a different domain. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVq-TuylYRDW"
      },
      "source": [
        "### Install huggingface transformers library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nhy3PzGQ44"
      },
      "source": [
        "Here, we will use the `transformers` [library](https://github.com/huggingface/transformers/) by huggingface. We'll start by installing the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQl0MMrOGIup"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "CoRc4z-vBYy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WThOUtpYvG-"
      },
      "source": [
        "### Load Fine-Tuned BERT-large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaweLnNXGhTY"
      },
      "source": [
        "For Question Answering we use the `BertForQuestionAnswering` class from the `transformers` library.\n",
        "\n",
        "Also, this class supports fine-tuning, but for this assignment we will load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
        "\n",
        "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html).\n",
        "\n",
        "For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. It has 24-layers and an embedding size of 1,024, for a total of 340M parameters. Altogether it is 1.34GB, so expect it to take a couple minutes to download.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mnv95sX-U9K"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imoOxoqGZ0h"
      },
      "source": [
        "Now, we have to load the tokenizer as well. \n",
        "\n",
        "Note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from `bert-base-uncased`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFQ5f7gv-RBH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MQ7b-GJIcM"
      },
      "source": [
        "Now we're ready to feed in an example.\n",
        "\n",
        "A QA example consists of a question and a passage of text containing the answer to that question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWzZP4EN-Zxg"
      },
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llLvxhScKLZn"
      },
      "source": [
        "We'll need to run the BERT tokenizer against both the `question` and the `answer_text`. To feed these into BERT, we actually concatenate them together and place the special [SEP] token in between.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoX33CfKGsr"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRVuaKSNFG8"
      },
      "source": [
        "Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iow838yPNDTv"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its 'id'.\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm208EApN16k"
      },
      "source": [
        "We've concatenated the `question` and `answer_text` together, but BERT still needs a way to distinguish them. BERT has two special \"Segment\" embeddings, one for segment \"A\" and one for segment \"B\". Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the `question` tokens, and the segment B embedding needs to be added to each of the `answer_text` tokens. \n",
        "\n",
        "These additions are handled for us by the `transformer` library, and all we need to do is specify a '0' or '1' for each token. \n",
        "\n",
        "Note: In the `transformers` library, huggingface likes to call these `token_type_ids`, but we will go with `segment_ids` since this seems clearer, and is consistent with the BERT paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ7PRx2dKqFN"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwhEw0kQPBN"
      },
      "source": [
        "Now, we will feed our example into the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQiKr6Aw-YTg"
      },
      "source": [
        "# Run our example through the model.\n",
        "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                             return_dict=True) \n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBdS_QkIbDzh"
      },
      "source": [
        "Further, we can highlight the answer just by looking at the most probable start and end words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUQ44hAJmn9"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twMUWmr2brRw"
      },
      "source": [
        "Here, we got the right output.\n",
        "\n",
        "Note: It's a little naive to pick the highest scores for start and end. The correct implementation is to pick the highest total score for which end >= start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6j2znkwXYsn"
      },
      "source": [
        "With a little more effort, we can reconstruct any words that got broken down into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khral6HZXCuI"
      },
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "    \n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "    \n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hh6nkIdXq-O"
      },
      "source": [
        "### Visualizing Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hG2YCHYXtg-"
      },
      "source": [
        "Now, let us see what the scores were for all of the words. The following cells generate bar plots showing the start and end scores for every word in the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkKFa73eJkPE"
      },
      "source": [
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "#sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7kazkb2iEuQ"
      },
      "source": [
        "Retrieve all of the start and end scores, and use all of the tokens as x-axis labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C56AtMg2UBxN"
      },
      "source": [
        "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
        "s_scores = start_scores.detach().numpy().flatten()\n",
        "e_scores = end_scores.detach().numpy().flatten()\n",
        "\n",
        "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "# to be unique, so we'll add the token index to the end of each one.\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaW7RyTiLeu"
      },
      "source": [
        "Create a bar plot showing the score for every input word being the \"start\" word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6OAV1dL3-UB"
      },
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('Start Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwrF7y6iS1l"
      },
      "source": [
        "Create a second bar plot showing the score for every input word being the \"end\" word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tXEqIp-Tzou"
      },
      "source": [
        "# Create a barplot showing the end word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('End Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awgi7Z_a9KSq"
      },
      "source": [
        "Let us also visualize both the start and end scores on a single bar plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4VUk6R05uXS"
      },
      "source": [
        "# Store the tokens and scores in a DataFrame. \n",
        "# Each token will have two rows, one for its start score and one for its end\n",
        "# score. The \"marker\" column will differentiate them. A little wacky, I know.\n",
        "scores = []\n",
        "for (i, token_label) in enumerate(token_labels):\n",
        "\n",
        "    # Add the token's start score as one row.\n",
        "    scores.append({'token_label': token_label, \n",
        "                   'score': s_scores[i],\n",
        "                   'marker': 'start'})\n",
        "    \n",
        "    # Add  the token's end score as another row.\n",
        "    scores.append({'token_label': token_label, \n",
        "                   'score': e_scores[i],\n",
        "                   'marker': 'end'})\n",
        "    \n",
        "df = pd.DataFrame(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xyo-I97Ntt"
      },
      "source": [
        "# Draw a grouped barplot to show start and end scores for each word.\n",
        "# The \"hue\" parameter is where we tell it which datapoints belong to which\n",
        "# of the two series.\n",
        "g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df,\n",
        "                kind=\"bar\", height=6, aspect=4)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "g.ax.grid(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWtcRpPef-Ce"
      },
      "source": [
        "Now, we will define a function for the QA process so we can easily try out other Question & Answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH8NbBlsfxZ_"
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer and prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example data through the model.\n",
        "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                    return_dict=True) \n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVlKTK-njWrX"
      },
      "source": [
        "As our reference text, we have taken the Abstract of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4VPq6FdjxyX"
      },
      "source": [
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEB654YCknYv"
      },
      "source": [
        "Here, we will ask BERT what its name stands for (the answer is in the first sentence of the abstract)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfntqRCBegGj"
      },
      "source": [
        "question = \"What does BERT stand for?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6HcijzxkTO9"
      },
      "source": [
        "From the same text, we will ask BERT about applications of itself.\n",
        "\n",
        "The answer to the question comes from this passage from the abstract: \n",
        "\n",
        "> \"BERT model can be finetuned with just one additional output layer to create state-of-the-art models for **a wide range of tasks, such as question answering and language inference,** without substantial taskspecific\n",
        "architecture modifications.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVNVGN5-gI06"
      },
      "source": [
        "question = \"What are some example applications of BERT?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will try out another text and ask the question."
      ],
      "metadata": {
        "id": "hdbsWN0qtgIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "metadata": {
        "id": "OWlbF8706tn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Where was the Auction held?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "metadata": {
        "id": "w7HIKBgx68U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"When was famous rhinestone-studded glove held?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "metadata": {
        "id": "Sr9PlTuRCBE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What did Jackson wear?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "metadata": {
        "id": "sw1iYlKgDbAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all about the use of BERT for Question & Answering task.  "
      ],
      "metadata": {
        "id": "l_YSPWQHuOsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERT's primary objective is to employ bidirectional training of the Transformer, a popular attention model, \n",
        "for the purpose of language modeling"
      ],
      "metadata": {
        "id": "OmFFiY8Vuzsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. State True or False: BERT's key objective is to employ bidirectional training of the Transformer, a popular attention model, for the purpose of language modeling. \n",
        "Answer1 = \"\" #@param [\"\",\"True\",\"False\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Select the True statement(s) from below options:\n",
        "Answer2 = \"\" #@param [\"\",\"BERT utilizes the Transformer, an attention mechanism that learns contextual relationships between words or sub-words in a text\", \"Since BERT’s goal is to generate a language model, and for this purpose, only the encoder mechanism is necessary.\",\"With the use of BERT, a Question & Answer (Q&A) model can be trained to identify the start and end positions of an answer by learning two additional vectors.\",\"All of the above\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}